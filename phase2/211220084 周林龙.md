## <center>科研实践-虚拟主播第二阶段技术报告</center>
<p align = right>211220084 周林龙</p>  

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [科研实践-虚拟主播第二阶段技术报告](#center科研实践-虚拟主播第二阶段技术报告center)
  - [1. 实现思路](#1-实现思路)
  - [2. 实现过程](#2-实现过程)
    - [2.1 虚拟主播形象与人设、](#21-虚拟主播形象与人设)
    - [2.2 获取直播间弹幕](#22-获取直播间弹幕)
    - [2.3 获取AI文本回复](#23-获取ai文本回复)
    - [2.4 转换语音回复](#24-转换语音回复)
  - [3. 效果展示](#3-效果展示)

<!-- /code_chunk_output -->

<p style="page-break-after:always;"></p>

### 1. 实现思路
具体虚拟主播的实现分为以下几部分：

- 获取live2D虚拟主播形象并构造其人设
- 获取直播间观众发送的弹幕
- 通过ChatGpt或其他大语言模型输入人设和弹幕，获取AI文本回复
- 通过TTS技术将AI的文本回复转为语音回复
- 进行直播间设置并开播

### 2. 实现过程
#### 2.1 虚拟主播形象与人设

这里我选择使用[B站up 一个ayabe 的live2d模型-简单小白猫](https://www.bilibili.com/video/BV14v4y1p7eR)，导入到VTube Studio后调整效果如下

![image-20240127170553779](\image\image-20240127170553779.png)



#### 2.2 获取直播间弹幕

最初选择按照段鑫宇、韩楚余同学示例中的[B站up澪式烧酒的教程视频](https://www.bilibili.com/video/BV1EM411T7PR/)进行操作，但实操安装配置完Python及相关环境后，运行start.py文件并不能正确将直播间的弹幕获取到终端上，如下图。
![image-20240122151832069](\image\image-20240122151832069.png)

并且获取弹幕并不是一定失败，不改变文件的情况下多次运行中存在成功获取弹幕的可能，但并没有排查出问题，最后看到这个UP使用的是[blivedm: 获取bilibili直播弹幕](https://github.com/xfgryujk/blivedm)的接口，我选择去github获取最新的版本进行直播间弹幕获取的测试，最后可以稳定的获取直播间弹幕，并且可以通过已登陆账号的cookie的SESSDATA来实现登陆，避免无法查看弹幕的用户名。

![image-20240122152125892](\image\image-20240122152125892.png)

![image-20240122152536944](\image\image-20240122152536944.png)

#### 2.3 获取AI文本回复

- **AI模型选择**：这里我选择使用智谱AI的GLM-4模型，接口文档在[智谱AI开放平台](https://open.bigmodel.cn/dev/api#glm-4)，接口提供了异步调用返回请求id，再根据请求id查询回复结果的方法。（其实我更想用智谱AI的CharacterGLM模型，这个超拟人大模型更适合虚拟主播，但是文档里的示例代码都跑不了）

- **异步处理消息**：blivedm在处理弹幕、礼物等消息时，为了保持处理消息的顺序以及避免因处理消息时过长的异步操作阻塞网络携程（处理消息和网络运行在同一个协程中），将处理消息做为同步函数，所以对于等待AI回复以及后续的转语音等待语音播放完毕，需要使用create_task处理异步操作。

- **上下文语境联系**：获取AI文本回复时，弹幕为一段一段的（如两条先后的弹幕“太阳系有哪些行星”，“最大的是哪个？”），必须联系弹幕上下文语境，才能获取到正确合理的两个回复，否则只会得到独立没有关联的两个问题的回复，我这里选择用`message_list`和`response_list`保存更新最近的若干条问答，与新的弹幕组合在一起对AI进行询问，获取相对合理的回复。(这样做的话，目前的虚拟主播就是个只有7秒钟记忆的主播，可以改进的方法是将每次的问答都进行保存在list中，最后存入文件中)

#### 2.4 转换语音回复并播放

- **语音转换**：使用edge-tts，将ai的回复转换为语音mp3文件

- **语音播放**：使用pygame.mixer，load储存的mp3文件并进行播放

- **虚拟声卡**：Vtube Studio需要音频输入来源作为相关参数，可以依靠这个修改皮套的口形与动作，所以我选择安装VoiceMeeter进行相关配置

  ![image-20240127190355573](\image\image-20240127190355573.png)

#### 2.5 直播间及相关设置

- **Blivechat**：同样利用Blivedm进行开发的工具，可以在直播间显示弹幕列表
- **VoiceMeeter**：修改麦克风设备，播放音乐和AI的回复

![image-20240127190705904](\image\image-20240127190705904.png)

### 3. 效果展示

[示例视频](test.mp4)为demo.mp4，下图终端的输出，~~AI说会表演才艺真的给我整乐了，字我都打错了hhhhh~~

![image-20240127191139610](\image\image-20240127191139610.png)

### 4.改进与提升

- **前文记忆**：虚拟主播对于前文的记忆仍需要改进
- **修改音色**：可以利用开源项目训练语音转语音的模型，将edge-tts的语音修改为新的目标音色
- **动作与表情**：可以利用VtubeStudio的API接口和已有的插件，编写程序控制虚拟主播的动作、表情等
